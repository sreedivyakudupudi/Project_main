{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    return 0\n",
    "  count=0\n",
    "  # crawls inside folders\n",
    "  for current_path,dirs,files in os.walk(directory):\n",
    "    for dr in dirs:\n",
    "      count+= len(glob.glob(os.path.join(current_path,dr+\"/*\")))\n",
    "  return count\n",
    "train_dir =r\"C:\\food\\pythonProject\\image data\\tomato\\train\"\n",
    "test_dir=r\"C:\\food\\pythonProject\\image data\\tomato\\val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Classes\n",
      "14529 Train images\n",
      "3631 Test images\n"
     ]
    }
   ],
   "source": [
    " #train file image count\n",
    "train_samples =get_files(train_dir)\n",
    "#to get tags\n",
    "num_classes=len(glob.glob(train_dir+\"/*\")) \n",
    "#test file image count\n",
    "test_samples=get_files(test_dir)\n",
    "print(num_classes,\"Classes\")\n",
    "print(train_samples,\"Train images\")\n",
    "print(test_samples,\"Test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14529 images belonging to 10 classes.\n",
      "Found 3631 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "input_shape=(256,256,3)\n",
    "train_generator =train_datagen.flow_from_directory(train_dir,target_size=(256,256),batch_size=32)\n",
    "test_generator=test_datagen.flow_from_directory(test_dir,shuffle=True,target_size=(256,256),batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3631 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                       test_dir, # same directory as training data\n",
    "                       target_size=(256,256),\n",
    "                       batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "56/56 [==============================] - 713s 13s/step - loss: 1.2063 - accuracy: 0.5898 - val_loss: 0.7166 - val_accuracy: 0.7634 - lr: 5.0000e-05\n",
      "Epoch 2/30\n",
      "56/56 [==============================] - 689s 12s/step - loss: 0.5923 - accuracy: 0.8080 - val_loss: 0.4421 - val_accuracy: 0.8549 - lr: 4.8333e-05\n",
      "Epoch 3/30\n",
      "56/56 [==============================] - 679s 12s/step - loss: 0.4045 - accuracy: 0.8610 - val_loss: 0.3639 - val_accuracy: 0.8839 - lr: 4.5111e-05\n",
      "Epoch 4/30\n",
      "56/56 [==============================] - 633s 11s/step - loss: 0.3060 - accuracy: 0.8968 - val_loss: 0.2771 - val_accuracy: 0.9085 - lr: 4.0600e-05\n",
      "Epoch 5/30\n",
      "56/56 [==============================] - 620s 11s/step - loss: 0.2658 - accuracy: 0.9085 - val_loss: 0.3968 - val_accuracy: 0.8638 - lr: 3.5187e-05\n",
      "Epoch 6/30\n",
      "56/56 [==============================] - 558s 10s/step - loss: 0.2096 - accuracy: 0.9291 - val_loss: 0.1949 - val_accuracy: 0.9353 - lr: 2.9322e-05\n",
      "Epoch 7/30\n",
      "56/56 [==============================] - 554s 10s/step - loss: 0.1837 - accuracy: 0.9375 - val_loss: 0.1766 - val_accuracy: 0.9487 - lr: 2.3458e-05\n",
      "Epoch 8/30\n",
      "56/56 [==============================] - 552s 10s/step - loss: 0.1535 - accuracy: 0.9487 - val_loss: 0.1467 - val_accuracy: 0.9464 - lr: 1.7984e-05\n",
      "Epoch 9/30\n",
      "56/56 [==============================] - 551s 10s/step - loss: 0.1671 - accuracy: 0.9448 - val_loss: 0.1342 - val_accuracy: 0.9643 - lr: 1.3188e-05\n",
      "Epoch 10/30\n",
      "56/56 [==============================] - 550s 10s/step - loss: 0.1318 - accuracy: 0.9581 - val_loss: 0.1327 - val_accuracy: 0.9464 - lr: 9.2319e-06\n",
      "Epoch 11/30\n",
      "56/56 [==============================] - 548s 10s/step - loss: 0.1257 - accuracy: 0.9608 - val_loss: 0.1590 - val_accuracy: 0.9509 - lr: 6.1546e-06\n",
      "Epoch 12/30\n",
      "56/56 [==============================] - 547s 10s/step - loss: 0.1139 - accuracy: 0.9637 - val_loss: 0.1411 - val_accuracy: 0.9487 - lr: 3.8979e-06\n",
      "Epoch 13/30\n",
      "56/56 [==============================] - 558s 10s/step - loss: 0.1011 - accuracy: 0.9693 - val_loss: 0.1665 - val_accuracy: 0.9442 - lr: 2.3388e-06\n",
      "Epoch 14/30\n",
      "56/56 [==============================] - 553s 10s/step - loss: 0.0931 - accuracy: 0.9676 - val_loss: 0.1200 - val_accuracy: 0.9598 - lr: 1.3253e-06\n",
      "Epoch 15/30\n",
      "56/56 [==============================] - 548s 10s/step - loss: 0.0878 - accuracy: 0.9749 - val_loss: 0.1206 - val_accuracy: 0.9665 - lr: 7.0682e-07\n",
      "Epoch 16/30\n",
      "56/56 [==============================] - 541s 10s/step - loss: 0.0991 - accuracy: 0.9632 - val_loss: 0.1213 - val_accuracy: 0.9576 - lr: 3.5341e-07\n",
      "Epoch 17/30\n",
      "56/56 [==============================] - 550s 10s/step - loss: 0.0904 - accuracy: 0.9699 - val_loss: 0.1264 - val_accuracy: 0.9576 - lr: 1.6493e-07\n",
      "Epoch 18/30\n",
      "56/56 [==============================] - 547s 10s/step - loss: 0.0886 - accuracy: 0.9721 - val_loss: 0.1353 - val_accuracy: 0.9531 - lr: 7.1468e-08\n",
      "Epoch 19/30\n",
      "56/56 [==============================] - 545s 10s/step - loss: 0.0751 - accuracy: 0.9771 - val_loss: 0.1658 - val_accuracy: 0.9554 - lr: 2.8587e-08\n",
      "Epoch 20/30\n",
      "56/56 [==============================] - 544s 10s/step - loss: 0.0886 - accuracy: 0.9727 - val_loss: 0.1525 - val_accuracy: 0.9509 - lr: 1.0482e-08\n",
      "Epoch 21/30\n",
      "56/56 [==============================] - 532s 10s/step - loss: 0.0718 - accuracy: 0.9818 - val_loss: 0.0857 - val_accuracy: 0.9688 - lr: 3.4940e-09\n",
      "Epoch 22/30\n",
      "56/56 [==============================] - 540s 10s/step - loss: 0.0821 - accuracy: 0.9738 - val_loss: 0.1248 - val_accuracy: 0.9554 - lr: 1.0482e-09\n",
      "Epoch 23/30\n",
      "56/56 [==============================] - 538s 10s/step - loss: 0.0951 - accuracy: 0.9738 - val_loss: 0.1080 - val_accuracy: 0.9665 - lr: 2.7952e-10\n",
      "Epoch 24/30\n",
      "56/56 [==============================] - 548s 10s/step - loss: 0.0944 - accuracy: 0.9671 - val_loss: 0.1239 - val_accuracy: 0.9576 - lr: 6.5221e-11\n",
      "Epoch 25/30\n",
      "56/56 [==============================] - 545s 10s/step - loss: 0.0881 - accuracy: 0.9710 - val_loss: 0.1221 - val_accuracy: 0.9598 - lr: 1.3044e-11\n",
      "Epoch 26/30\n",
      "56/56 [==============================] - 548s 10s/step - loss: 0.0822 - accuracy: 0.9732 - val_loss: 0.1142 - val_accuracy: 0.9554 - lr: 2.1740e-12\n",
      "Epoch 27/30\n",
      "56/56 [==============================] - 538s 10s/step - loss: 0.0827 - accuracy: 0.9732 - val_loss: 0.0791 - val_accuracy: 0.9710 - lr: 2.8987e-13\n",
      "Epoch 28/30\n",
      "56/56 [==============================] - 538s 10s/step - loss: 0.0977 - accuracy: 0.9682 - val_loss: 0.0992 - val_accuracy: 0.9732 - lr: 2.8987e-14\n",
      "Epoch 29/30\n",
      "56/56 [==============================] - 532s 10s/step - loss: 0.0814 - accuracy: 0.9727 - val_loss: 0.1241 - val_accuracy: 0.9598 - lr: 1.9325e-15\n",
      "Epoch 30/30\n",
      "56/56 [==============================] - 540s 10s/step - loss: 0.0859 - accuracy: 0.9743 - val_loss: 0.0998 - val_accuracy: 0.9710 - lr: 6.4416e-17\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-5\n",
    "train_batch_size = 256\n",
    "eval_batch_size = 256\n",
    "num_epochs = 30\n",
    "num_classes = 10  # Replace with the actual number of classes in your dataset\n",
    "\n",
    "# Load pretrained VGG16 model\n",
    "pretrained_model = VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(256, 256, 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers up to a specific layer\n",
    "freeze_until_layer = 'block4_conv3'\n",
    "for layer in pretrained_model.layers:\n",
    "    if layer.name == freeze_until_layer:\n",
    "        break\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = Flatten()(pretrained_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model2 = Model(inputs=pretrained_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model2.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    return lr * (1 - epoch / num_epochs)\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model\n",
    "history = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=train_generator.samples // train_batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // eval_batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[lr_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 650s 6s/step - loss: 0.1393 - accuracy: 0.9554\n",
      "Test Loss: 0.1393280029296875\n",
      "Test Accuracy: 0.9553841948509216\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model2.evaluate(test_generator)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"tomato.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
